{"title":"SiamDW","slug":"SiamDW","date":"2020-03-04T15:22:25.000Z","updated":"2020-03-04T15:43:32.599Z","comments":true,"path":"api/articles/SiamDW.json","photos":[],"link":"","excerpt":"Deeper and Wider Siamese Networks for Real-Time Visual Tracking","covers":["/2020/03/04/SiamDW/simple_siam.png","/2020/03/04/SiamDW/SINT.png","/2020/03/04/SiamDW/SiamFC_1.png","/2020/03/04/SiamDW/SiamRPN_net.png","/2020/03/04/SiamDW/motivation.png","/2020/03/04/SiamDW/padding_test.png","/2020/03/04/SiamDW/without_padding.png","/2020/03/04/SiamDW/with_padding.png","/2020/03/04/SiamDW/other_test.png","/2020/03/04/SiamDW/analysis.png","/2020/03/04/SiamDW/1times1conv.png","/2020/03/04/SiamDW/CIR.png","/2020/03/04/SiamDW/CIR-D.png","/2020/03/04/SiamDW/CIR-D_2.png","/2020/03/04/SiamDW/loss_feature_with_str-1583335627710.png"],"content":"<p>Deeper and Wider Siamese Networks for Real-Time Visual Tracking</p>\n<a id=\"more\"></a>\n<p>[toc]</p>\n<p><a href=\"https://www.bilibili.com/video/av52113951\" target=\"_blank\" rel=\"noopener\">作者讲解视频&amp;PPT</a></p>\n<h1 id=\"siam特点\"><a class=\"markdownIt-Anchor\" href=\"#siam特点\"></a> siam特点</h1>\n<ol>\n<li>共享权重</li>\n<li>做检索，对比相似性。information经过相同网络，在空间中产生表征向量，计算距离代表相似性</li>\n<li>以pair对形式进入网络，两两组合，相当于增加数据量</li>\n</ol>\n<p><img src=\"/2020/03/04/SiamDW/simple_siam.png\" alt=\"simple_siam\"></p>\n<h1 id=\"siam历史\"><a class=\"markdownIt-Anchor\" href=\"#siam历史\"></a> Siam历史</h1>\n<ol>\n<li>\n<p>SINT：在图像中抽取很多候选目标，经过同一网络，与模板比较，选择距离最小（最相似）</p>\n<p><img src=\"/2020/03/04/SiamDW/SINT.png\" alt=\"SINT\"></p>\n</li>\n<li>\n<p>SiamFC：Cross-correlation互相关，与卷积类似，滑窗，计算相似性</p>\n<p><img src=\"/2020/03/04/SiamDW/SiamFC_1.png\" alt=\"SiamFC_1\"></p>\n</li>\n<li>\n<p>SiamRPN：通过RPN，解决了SiamFC中的僵硬的尺度估计：手动设定几个scale</p>\n<p><img src=\"/2020/03/04/SiamDW/SiamRPN_net.png\" alt=\"SiamRPN_net\"></p>\n</li>\n</ol>\n<h1 id=\"motivation\"><a class=\"markdownIt-Anchor\" href=\"#motivation\"></a> Motivation</h1>\n<p>原有backbone很浅，改用深层，performance降低及思考：</p>\n<ol>\n<li>backbone的基本模块：res、inception、卷积</li>\n<li>stride：分类大，精细任务小</li>\n<li>padding：SiamFC中padding全部去掉，而深层backbone的padding是必要的</li>\n<li>输出尺寸：AlexNet 6×6、而后者为16或32</li>\n</ol>\n<p><img src=\"/2020/03/04/SiamDW/motivation.png\" alt=\"motivation\"></p>\n<h1 id=\"analysis-guidelines\"><a class=\"markdownIt-Anchor\" href=\"#analysis-guidelines\"></a> Analysis &amp; Guidelines</h1>\n<p>通过修改感受野、步长、输出特征尺寸、padding与否、backbone类型，进行实验，发现padding会降点</p>\n<h2 id=\"paddingpad\"><a class=\"markdownIt-Anchor\" href=\"#paddingpad\"></a> padding/PAD</h2>\n<p><img src=\"/2020/03/04/SiamDW/padding_test.png\" alt=\"padding_test\"></p>\n<p>padding导致掉点（但输出尺寸不一致，不能说完全都是padding影响）</p>\n<ol>\n<li>\n<p>无padding：</p>\n<p><img src=\"/2020/03/04/SiamDW/without_padding.png\" alt=\"without_padding\"></p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow></mrow><annotation encoding=\"application/x-tex\"></annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"></span></span></span></p>\n<p>R2=\\phi(B)·\\phi(E) \\<br>\nR1 = R2</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow></mrow><annotation encoding=\"application/x-tex\">\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"></span></span></span></p>\n<ol start=\"2\">\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>A</mi><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">A B</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">A</span><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span></span></span></span>：目标位置</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding=\"application/x-tex\">\\phi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">ϕ</span></span></span></span>：backbone，提取特征向量</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo separator=\"true\">⋅</mo></mrow><annotation encoding=\"application/x-tex\">·</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.44445em;vertical-align:0em;\"></span><span class=\"mpunct\">⋅</span></span></span></span>：cross-correlation</li>\n<li><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.00773em;\">R</span></span></span></span>：响应</li>\n<li>因为E相同、A和B是同一目标提取向量所得，相同，故两响应相同，满足平移不变性</li>\n</ol>\n</li>\n<li>\n<p>有padding：</p>\n<p><img src=\"/2020/03/04/SiamDW/with_padding.png\" alt=\"with_padding\"></p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow></mrow><annotation encoding=\"application/x-tex\"></annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"></span></span></span></p>\n<p>R2=\\phi(B’)·\\phi(E’) \\<br>\nR1 \\not= R2</p>\n<p class=\"katex-block\"><span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow></mrow><annotation encoding=\"application/x-tex\">\n</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"></span></span></span></p>\n<ol start=\"2\">\n<li>扩大后的目标<strong>仍在</strong>画面中时，没有变化</li>\n<li>扩大后的目标<strong>不在</strong>画面中时，<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>ϕ</mi><mo stretchy=\"false\">(</mo><msup><mi>A</mi><mo mathvariant=\"normal\">′</mo></msup><mo stretchy=\"false\">)</mo><mo>≠</mo><mi>ϕ</mi><mo stretchy=\"false\">(</mo><msup><mi>B</mi><mo mathvariant=\"normal\">′</mo></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\phi(A&#x27;)\\not=\\phi(B&#x27;)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.001892em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">ϕ</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">A</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\"><span class=\"mord\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"rlap\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"inner\"><span class=\"mrel\"></span></span><span class=\"fix\"></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.36687em;vertical-align:0em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.001892em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">ϕ</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.05017em;\">B</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>，最终导致两响应不同，破坏平移不变性</li>\n<li><strong>而深层网络中，感受野会快速扩大，很容易就会超出画面</strong></li>\n</ol>\n</li>\n</ol>\n<h2 id=\"步长str\"><a class=\"markdownIt-Anchor\" href=\"#步长str\"></a> 步长STR</h2>\n<p><img src=\"/2020/03/04/SiamDW/other_test.png\" alt=\"other_test\"></p>\n<p>跟踪的位移较小，使用小步长捕捉微小变化（siam跟踪通常为8）</p>\n<h2 id=\"感受野rf和输出尺寸ofs\"><a class=\"markdownIt-Anchor\" href=\"#感受野rf和输出尺寸ofs\"></a> 感受野RF和输出尺寸OFS</h2>\n<ol>\n<li>\n<p>二者具有相关性，互影响，所以一起讨论</p>\n</li>\n<li>\n<p>在实验中，performance出现“单峰”，表示RF和OFS大小适中为好</p>\n</li>\n<li>\n<p>后续分析：</p>\n<p><img src=\"/2020/03/04/SiamDW/analysis.png\" alt=\"analysis\"></p>\n<ol>\n<li>feature map中的element，所对应的RF太小，表示网络太浅，提取特征能力差；而RF太大，不仅捕捉不到细节，而且element之间的重叠率overlap ratio上升，导致特征信息冗余，也会降低feature的判别</li>\n<li>故感受野合适大小才行</li>\n</ol>\n</li>\n</ol>\n<h2 id=\"guidelines\"><a class=\"markdownIt-Anchor\" href=\"#guidelines\"></a> Guidelines</h2>\n<ol>\n<li>步长：小，4或8，捕捉运动变化（4较慢）</li>\n<li>感受野：feature与输入的尺寸比在60%~80%</li>\n<li>步长、感受野、输出尺寸，应作为整体考虑，最终performance是每个环节共同决定。实验中不去掉padding，而通过CIR修改输出尺寸，也能获得不错的效果</li>\n</ol>\n<h1 id=\"method\"><a class=\"markdownIt-Anchor\" href=\"#method\"></a> Method</h1>\n<p>根据以上guidelines，设计新的网络</p>\n<ol>\n<li>\n<p>1×1卷积：调整维度，减少参数</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/27642620\" target=\"_blank\" rel=\"noopener\" title=\"源网址\"><img src=\"/2020/03/04/SiamDW/1times1conv.png\" alt=\"1times1conv\"></a></p>\n</li>\n<li>\n<p>cropping-inside residual unit</p>\n<ol>\n<li>\n<p>CIR Module：原始残差单元中，两个1×1用于维度调整，3×3卷积，因为padding为1、stride为1，输出feature map中仅最外一圈受padding影响，故本文中移去。此法除了消除padding影响，而且因为减小了feature大小，有提速作用。</p>\n<p><img src=\"/2020/03/04/SiamDW/CIR.png\" alt=\"CIR\"></p>\n</li>\n<li>\n<p>CIR-Downsampling Module：原始残差网络的深层下采样单元，采用conv中增大步长的方式减小feature map大小；本文不同于传统认为的conv+大步长会减轻信息丢失，而是采用了小步长+最大池化的下采样方法。</p>\n<p><img src=\"/2020/03/04/SiamDW/CIR-D.png\" alt=\"CIR-D\"></p>\n<p>原因：传统方法结合crop消去最外层feature后，剩余feature map的感受野不足以覆盖完整图片。改为stride1后，紫色映射范围向左上移动一位，覆盖完全。</p>\n<p>以原本的stride2 conv3的方式下采样：</p>\n<p><img src=\"/2020/03/04/SiamDW/CIR-D_2.png\" alt=\"CIR-D_2\"></p>\n<p><img src=\"/2020/03/04/SiamDW/loss_feature_with_str-1583335627710.png\" alt=\"loss_feature_with_str\"></p>\n</li>\n</ol>\n</li>\n<li>\n<p>构建新网络</p>\n<p>通过在不同网络、不同深度下的组合测试（实验中注意不能一味追求深度，要注意输出尺寸）</p>\n<ol>\n<li>确定步长：3-stage网络步长为8；2-stage网络步长为4</li>\n<li>堆叠CIR单元：合理控制CIR、CIR-D单元数，确保最终层神经元感受野在原图的60%~80%（并非全部都是CIR单元，具体见论文描述）</li>\n<li>深度增加，感受野超过合理区间时，减半步长至4</li>\n</ol>\n<p>最终确定了深度为22层。</p>\n</li>\n<li>\n<p>加宽channel</p>\n<p>作者尝试在ResNet的unit的1、 3、 1卷积中加宽通道数以提升性能，结果过高的通道数造成掉点。作者认为过高的通道对cross-correlation并不友好，最终在256或512为宜。</p>\n</li>\n</ol>\n","categories":[],"tags":[{"name":"AI","slug":"AI","count":1,"path":"api/tags/AI.json"}]}